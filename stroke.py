# -*- coding: utf-8 -*-
"""Stroke

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z-OZ2dOUMgNMbgYEDYE4oS6GXcv0pRRA
"""

#NN
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report


stroke_data = pd.read_csv('stroke_data.csv')

# missing values in the 'bmi' column by filling them with the mean value
stroke_data['bmi'].fillna(stroke_data['bmi'].mean(), inplace=True)

# categorical variables to numerical LabelEncoder
le = LabelEncoder()
stroke_data['gender'] = le.fit_transform(stroke_data['gender'])

#split
y = stroke_data["stroke"]
X = stroke_data.drop(['id', 'stroke'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# OneHotEncoder
encoder = OneHotEncoder()
X_train_encoded = encoder.fit_transform(X_train.select_dtypes(include=['object'])).toarray()
X_test_encoded = encoder.transform(X_test.select_dtypes(include=['object'])).toarray()

# Concatenate
X_train_encoded = np.concatenate([X_train_encoded, X_train.select_dtypes(include=[np.number]).values], axis=1)
X_test_encoded = np.concatenate([X_test_encoded, X_test.select_dtypes(include=[np.number]).values], axis=1)

# StandardScaler to the encoded data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_encoded)
X_test_scaled = scaler.transform(X_test_encoded)

# NNN + GridSearchCV for hyperparameter
parameters = {
    'solver': ['adam'],
    'alpha': [0.0001, 0.001, 0.01],
    'hidden_layer_sizes': [(100,), (100, 100), (200, 100, 50)],
    'activation': ['relu', 'tanh'],
    'random_state': [42],
    'learning_rate': ['constant', 'invscaling', 'adaptive'],
}

neural_model = MLPClassifier(max_iter=1000, early_stopping=True, random_state=42, verbose=True, validation_fraction=0.1)

# implementing k-fold cross-validation for better eval
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

grid_search = GridSearchCV(neural_model, parameters, n_jobs=-1, cv=kfold)
grid_search.fit(X_train_scaled, y_train)

# best hyperparameters
print("Best Hyperparameters:")
print(grid_search.best_params_)

# Get best neural model from the GridSearchCV
neural_model = grid_search.best_estimator_

# Confusion Matrix for NNN
neural_cm = confusion_matrix(y_test, neural_predictions)
sns.heatmap(neural_cm, annot=True, fmt='d', cmap='Blues')
plt.title('Neural Network Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Classification
print("Classification Report:")
print(classification_report(y_test, neural_predictions))

# Predictions + accuracy
neural_predictions = neural_model.predict(X_test_scaled)
neural_accuracy = accuracy_score(y_test, neural_predictions)
print('Neural Network Model Accuracy: {:.2%}'.format(neural_accuracy))

#RandomForest
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
stroke_data = pd.read_csv('stroke_data.csv')

# Missing values in the 'bmi' column by filling them with the mean value
stroke_data['bmi'].fillna(stroke_data['bmi'].mean(), inplace=True)

# Categorical variables to numerical using LabelEncoder
le = LabelEncoder()
stroke_data['gender'] = le.fit_transform(stroke_data['gender'])

# Split the data
y = stroke_data["stroke"]
X = stroke_data.drop(['id', 'stroke'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# OneHotEncoder
encoder = OneHotEncoder()
X_train_encoded = encoder.fit_transform(X_train.select_dtypes(include=['object'])).toarray()
X_test_encoded = encoder.transform(X_test.select_dtypes(include=['object'])).toarray()

# Concatenate
X_train_encoded = np.concatenate([X_train_encoded, X_train.select_dtypes(include=[np.number]).values], axis=1)
X_test_encoded = np.concatenate([X_test_encoded, X_test.select_dtypes(include=[np.number]).values], axis=1)

# StandardScaler to the encoded data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_encoded)
X_test_scaled = scaler.transform(X_test_encoded)
parameters = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'random_state': [42],
    'class_weight': ['balanced', 'balanced_subsample'], # This helps handle imbalanced data
}

rf_model = RandomForestClassifier()

# Implementing k-fold cross-validation for better evaluation
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

grid_search = GridSearchCV(rf_model, parameters, n_jobs=-1, cv=kfold)
grid_search.fit(X_train_scaled, y_train)

# Best hyperparameters
print("Best Hyperparameters:")
print(grid_search.best_params_)

# Get the best Random Forest model from the GridSearchCV
rf_model = grid_search.best_estimator_
# Predictions + accuracy
rf_predictions = rf_model.predict(X_test_scaled)
rf_accuracy = accuracy_score(y_test, rf_predictions)
print('Random Forest Model Accuracy: {:.2%}'.format(rf_accuracy))

# Confusion Matrix for Random Forest
rf_cm = confusion_matrix(y_test, rf_predictions)
sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues')
plt.title('Random Forest Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Classification Report
print("Classification Report:")
print(classification_report(y_test, rf_predictions))
# Predictions + accuracy
rf_predictions = rf_model.predict(X_test_scaled)
rf_accuracy = accuracy_score(y_test, rf_predictions)
print('Random Forest Model Accuracy: {:.2%}'.format(rf_accuracy))

# Confusion Matrix for Random Forest
rf_cm = confusion_matrix(y_test, rf_predictions)
sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues')
plt.title('Random Forest Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Classification Report
print("Classification Report:")
print(classification_report(y_test, rf_predictions))

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn import svm
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

# Load the stroke data from a CSV file
stroke_data = pd.read_csv('stroke_data.csv')

# Explore the data
print(stroke_data.shape)  # Display the shape of the dataframe (rows, columns)
print(stroke_data.isnull().sum())  # Check for missing values in the dataset

# Handling missing values in the 'bmi' column by filling them with the mean value
stroke_data['bmi'].fillna(stroke_data['bmi'].mean(), inplace=True)

# Convert categorical variables to numerical using LabelEncoder
le = LabelEncoder()
stroke_data['gender'] = le.fit_transform(stroke_data['gender'])

# Visualize the data using swarmplot
sns.set_theme(style="whitegrid", palette="muted")
stroke_plot = sns.swarmplot(data=stroke_data, x="avg_glucose_level", y="gender", hue="stroke")
stroke_plot.set(ylabel="")
plt.show()

# Prepare the data for modeling
y = stroke_data["stroke"]
X = stroke_data.drop(['id', 'stroke'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# Logistic Regression Model
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)
logistic_predictions = logistic_model.predict(X_test)
logistic_accuracy = accuracy_score(y_test, logistic_predictions)
print('Logistic Regression Model Accuracy: {:.2%}'.format(logistic_accuracy))

# Confusion Matrix for Logistic Regression
logistic_cm = confusion_matrix(y_test, logistic_predictions)
sns.heatmap(logistic_cm, annot=True, fmt='d', cmap='Blues')
plt.title('Logistic Regression Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Support Vector Machine (SVM) Model
svm_model = svm.SVC()
svm_model.fit(X_train, y_train)
svm_predictions = svm_model.predict(X_test)
svm_accuracy = accuracy_score(y_test, svm_predictions)
print('Support Vector Machine Model Accuracy: {:.2%}'.format(svm_accuracy))

# Neural Network Model
neural_model = MLPClassifier(solver='adam', alpha=1e-1, hidden_layer_sizes=(100, 100, 100), random_state=80, learning_rate='constant')
neural_model.fit(X_train, y_train)
neural_predictions = neural_model.predict(X_test)
neural_accuracy = accuracy_score(y_test, neural_predictions)
print('Neural Network Model Accuracy: {:.2%}'.format(neural_accuracy))

# Confusion Matrix for Neural Network Model
neural_cm = confusion_matrix(y_test, neural_predictions)
sns.heatmap(neural_cm, annot=True, fmt='d', cmap='Blues')
plt.title('Neural Network Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score, confusion_matrix, classification_report,
    roc_auc_score, roc_curve, precision_recall_curve, f1_score
)

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from imblearn.over_sampling import SMOTE

# Load Data
stroke_data = pd.read_csv("stroke_data.csv")

# Handle missing values
stroke_data['bmi'].fillna(stroke_data['bmi'].mean(), inplace=True)

# Encode "gender" (binary categorical)
le = LabelEncoder()
stroke_data['gender'] = le.fit_transform(stroke_data['gender'])

# Split
y = stroke_data['stroke']
X = stroke_data.drop(['id', 'stroke'], axis=1)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=42, stratify=y
)

# Handle Class Imbalance with SMOTE
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)


# Preprocessing: OneHot + Scaling
categorical_features = X.select_dtypes(include=['object']).columns
numeric_features = X.select_dtypes(include=[np.number]).columns

preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
        ("num", StandardScaler(), numeric_features)
    ]
)

# Helper Function: Evaluation
def evaluate_model(name, model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])

    print(f"\n{name} Results:")
    print(classification_report(y_test, y_pred, digits=4))
    print(f"Accuracy: {acc:.4f}, F1: {f1:.4f}, ROC-AUC: {roc_auc:.4f}")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title(f"{name} Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1])
    plt.plot(fpr, tpr, label=f"{name} (AUC={roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], "k--")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"{name} ROC Curve")
    plt.legend()
    plt.show()

    return acc, f1, roc_auc

# Models
models = {
    "Logistic Regression": Pipeline([("preprocess", preprocessor),
                                     ("clf", LogisticRegression(max_iter=1000, class_weight="balanced"))]),

    "Random Forest": Pipeline([("preprocess", preprocessor),
                               ("clf", RandomForestClassifier(n_estimators=200, random_state=42, class_weight="balanced"))]),

    "Neural Network": Pipeline([("preprocess", preprocessor),
                                ("clf", MLPClassifier(hidden_layer_sizes=(100, 100),
                                                      max_iter=500, random_state=42,
                                                      early_stopping=True))]),

    "SVM": Pipeline([("preprocess", preprocessor),
                     ("clf", SVC(probability=True, class_weight="balanced", random_state=42))]),
}

# Evaluate all models
#
results = {}
for name, model in models.items():
    acc, f1, roc_auc = evaluate_model(name, model, X_train, y_train, X_test, y_test)
    results[name] = {"Accuracy": acc, "F1": f1, "ROC-AUC": roc_auc}

# Compare Performance
results_df = pd.DataFrame(results).T
print("\nModel Comparison:\n", results_df)

results_df.plot(kind="bar", figsize=(10, 6))
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.xticks(rotation=45)
plt.legend(loc="best")
plt.show()

# Feature Importance (Random Forest Example)
rf = models["Random Forest"].named_steps["clf"]
ohe = models["Random Forest"].named_steps["preprocess"].named_transformers_["cat"]
feature_names = list(ohe.get_feature_names_out(categorical_features)) + list(numeric_features)

importances = pd.Series(rf.feature_importances_, index=feature_names).sort_values(ascending=False).head(15)
importances.plot(kind="barh", figsize=(8,6))
plt.title("Top 15 Feature Importances (Random Forest)")
plt.show()

import shap

# SHAP for Explainability

# Use a small sample of X_test for faster SHAP computations
sample_size = 200
X_sample = X_test.sample(sample_size, random_state=42)

# Get preprocessor for feature names
ohe = models["Random Forest"].named_steps["preprocess"].named_transformers_["cat"]
feature_names = list(ohe.get_feature_names_out(categorical_features)) + list(numeric_features)

# Logistic Regression SHAP

print("\n=== Logistic Regression SHAP Explainability ===")
log_model = models["Logistic Regression"]
log_model.fit(X_train, y_train)

# Preprocess test data
X_sample_preprocessed = log_model.named_steps["preprocess"].transform(X_sample)

explainer_log = shap.LinearExplainer(
    log_model.named_steps["clf"],
    X_sample_preprocessed,
    feature_names=feature_names
)
shap_values_log = explainer_log(X_sample_preprocessed)

# Summary plot (global feature importance)
shap.summary_plot(shap_values_log, X_sample_preprocessed, feature_names=feature_names)

# Force plot for first sample (local explanation)
shap.plots.force(shap_values_log[0], matplotlib=True)


# ==============================
# Neural Network SHAP
# ==============================
print("\n=== Neural Network SHAP Explainability ===")
nn_model = models["Neural Network"]
nn_model.fit(X_train, y_train)

X_sample_preprocessed = nn_model.named_steps["preprocess"].transform(X_sample)

# KernelExplainer works for black-box models (slower but general)
explainer_nn = shap.KernelExplainer(
    nn_model.named_steps["clf"].predict_proba,
    shap.sample(X_sample_preprocessed, 100)  # background dataset
)
shap_values_nn = explainer_nn.shap_values(X_sample_preprocessed[:50])  # explain 50 instances

# Summary plot for Neural Net
shap.summary_plot(shap_values_nn[1], X_sample_preprocessed[:50], feature_names=feature_names)

# Force plot for one prediction
shap.plots.force(shap_values_nn[1][0], matplotlib=True)

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score, confusion_matrix, classification_report,
    roc_auc_score, roc_curve, precision_recall_curve, f1_score, average_precision_score
)

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from imblearn.over_sampling import SMOTE
import shap

# Load Data
stroke_data = pd.read_csv("stroke_data.csv")

# Handle missing values
stroke_data['bmi'].fillna(stroke_data['bmi'].mean(), inplace=True)

# Encode "gender" (binary categorical)
le = LabelEncoder()
stroke_data['gender'] = le.fit_transform(stroke_data['gender'])

# Split
y = stroke_data['stroke']
X = stroke_data.drop(['id', 'stroke'], axis=1)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=42, stratify=y
)

# Handle Class Imbalance with SMOTE
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Preprocessing: OneHot + Scaling
categorical_features = X.select_dtypes(include=['object']).columns
numeric_features = X.select_dtypes(include=[np.number]).columns

preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
        ("num", StandardScaler(), numeric_features)
    ]
)

# Helper Function: Evaluation
def evaluate_model(name, model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    # Metrics
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_proba)
    pr_auc = average_precision_score(y_test, y_proba)

    print(f"\n{name} Results:")
    print(classification_report(y_test, y_pred, digits=4))
    print(f"Accuracy: {acc:.4f}, F1: {f1:.4f}, ROC-AUC: {roc_auc:.4f}, PR-AUC: {pr_auc:.4f}")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title(f"{name} Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    plt.plot(fpr, tpr, label=f"{name} (ROC-AUC={roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], "k--")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"{name} ROC Curve")
    plt.legend()
    plt.show()

    # Precision-Recall Curve
    precision, recall, _ = precision_recall_curve(y_test, y_proba)
    plt.plot(recall, precision, label=f"{name} (PR-AUC={pr_auc:.2f})")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title(f"{name} Precision-Recall Curve")
    plt.legend()
    plt.show()

    return acc, f1, roc_auc, pr_auc

# Models
models = {
    "Logistic Regression": Pipeline([("preprocess", preprocessor),
                                     ("clf", LogisticRegression(max_iter=1000, class_weight="balanced"))]),

    "Random Forest": Pipeline([("preprocess", preprocessor),
                               ("clf", RandomForestClassifier(n_estimators=200, random_state=42, class_weight="balanced"))]),

    "Neural Network": Pipeline([("preprocess", preprocessor),
                                ("clf", MLPClassifier(hidden_layer_sizes=(100, 100),
                                                      max_iter=500, random_state=42,
                                                      early_stopping=True))]),

    "SVM": Pipeline([("preprocess", preprocessor),
                     ("clf", SVC(probability=True, class_weight="balanced", random_state=42))]),
}

# Evaluate all models
results = {}
for name, model in models.items():
    acc, f1, roc_auc, pr_auc = evaluate_model(name, model, X_train, y_train, X_test, y_test)
    results[name] = {"Accuracy": acc, "F1": f1, "ROC-AUC": roc_auc, "PR-AUC": pr_auc}

# Compare Models
results_df = pd.DataFrame(results).T
print("\nModel Comparison:\n", results_df)

results_df.plot(kind="bar", figsize=(12, 6))
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.xticks(rotation=45)
plt.legend(loc="best")
plt.show()

# Feature Importance (Random Forest Example)
rf = models["Random Forest"].named_steps["clf"]
ohe = models["Random Forest"].named_steps["preprocess"].named_transformers_["cat"]
feature_names = list(ohe.get_feature_names_out(categorical_features)) + list(numeric_features)

importances = pd.Series(rf.feature_importances_, index=feature_names).sort_values(ascending=False).head(15)
importances.plot(kind="barh", figsize=(8,6))
plt.title("Top 15 Feature Importances (Random Forest)")
plt.show()

# SHAP Explainability
sample_size = 200
X_sample = X_test.sample(sample_size, random_state=42)

# Logistic Regression SHAP
print("\n=== Logistic Regression SHAP Explainability ===")
log_model = models["Logistic Regression"]
log_model.fit(X_train, y_train)
X_sample_preprocessed = log_model.named_steps["preprocess"].transform(X_sample)

explainer_log = shap.LinearExplainer(
    log_model.named_steps["clf"],
    X_sample_preprocessed,
    feature_names=feature_names
)
shap_values_log = explainer_log(X_sample_preprocessed)

shap.summary_plot(shap_values_log, X_sample_preprocessed, feature_names=feature_names)
shap.plots.force(shap_values_log[0], matplotlib=True)

# Neural Network SHAP
print("\n=== Neural Network SHAP Explainability ===")
nn_model = models["Neural Network"]
nn_model.fit(X_train, y_train)
X_sample_preprocessed = nn_model.named_steps["preprocess"].transform(X_sample)

explainer_nn = shap.KernelExplainer(
    nn_model.named_steps["clf"].predict_proba,
    shap.sample(X_sample_preprocessed, 100)
)
shap_values_nn = explainer_nn.shap_values(X_sample_preprocessed[:50])

shap.summary_plot(shap_values_nn[1], X_sample_preprocessed[:50], feature_names=feature_names)
shap.plots.force(shap_values_nn[1][0], matplotlib=True)

# Stacking Ensemble
print("\n=== Stacking Ensemble Model ===")
estimators = [
    ('lr', models["Logistic Regression"].named_steps["clf"]),
    ('rf', models["Random Forest"].named_steps["clf"]),
    ('nn', models["Neural Network"].named_steps["clf"]),
    ('svm', models["SVM"].named_steps["clf"])
]

stacking_model = Pipeline([
    ("preprocess", preprocessor),
    ("clf", StackingClassifier(
        estimators=estimators,
        final_estimator=LogisticRegression(max_iter=1000, class_weight="balanced"),
        passthrough=True,
        cv=5,
        n_jobs=-1
    ))
])

acc, f1, roc_auc, pr_auc = evaluate_model("Stacking Ensemble", stacking_model, X_train, y_train, X_test, y_test)
results["Stacking Ensemble"] = {"Accuracy": acc, "F1": f1, "ROC-AUC": roc_auc, "PR-AUC": pr_auc}

# Final comparison
results_df = pd.DataFrame(results).T
print("\nFinal Model Comparison:\n", results_df)